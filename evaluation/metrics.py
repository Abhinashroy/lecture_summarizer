"""
Evaluation Metrics System
Implements comprehensive metrics framework for measuring system quality and performance.
"""

import time
import json
import os
from typing import Dict, List, Any, Tuple, Optional
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re
from collections import defaultdict
import logging

import config

class EvaluationMetrics:
    """
    Comprehensive evaluation system for measuring the quality and performance
    of the lecture summarization system across multiple dimensions.
    """
    
    def __init__(self):
        self.logger = logging.getLogger("EvaluationMetrics")
        self.embedding_model = None
        self.evaluation_history = []
        self._initialize_evaluation_system()
    
    def _initialize_evaluation_system(self):
        """Initialize the evaluation system components."""
        try:
            # Load embedding model for semantic similarity
            self.embedding_model = SentenceTransformer(config.EMBEDDING_MODEL)
            self.logger.info("Evaluation system initialized successfully")
        except Exception as e:
            self.logger.error(f"Failed to initialize evaluation system: {e}")
            self.embedding_model = None
    
    def evaluate_transcription_accuracy(self, predicted_text: str, 
                                      reference_text: str = None) -> Dict[str, float]:
        """
        Evaluate transcription accuracy using multiple metrics.
        
        Args:
            predicted_text: Transcribed text from the system
            reference_text: Ground truth transcription (if available)
            
        Returns:
            Dict containing accuracy metrics
        """
        metrics = {}
        
        if reference_text:
            # Calculate Word Error Rate (WER)
            wer = self._calculate_wer(predicted_text, reference_text)
            metrics["word_error_rate"] = wer
            metrics["word_accuracy"] = 1.0 - wer
            
            # Calculate semantic similarity
            semantic_sim = self._calculate_semantic_similarity(predicted_text, reference_text)
            metrics["semantic_similarity"] = semantic_sim
            
            # Calculate BLEU score
            bleu = self._calculate_bleu_score(predicted_text, reference_text)
            metrics["bleu_score"] = bleu
        
        # Quality indicators (reference-free)
        metrics.update(self._assess_transcription_quality(predicted_text))
        
        return metrics
    
    def evaluate_concept_extraction(self, extracted_concepts: List[Dict], 
                                  reference_concepts: List[str] = None,
                                  original_text: str = "") -> Dict[str, float]:
        """
        Evaluate concept extraction quality.
        
        Args:
            extracted_concepts: Concepts extracted by the system
            reference_concepts: Ground truth concepts (if available)
            original_text: Original lecture text
            
        Returns:
            Dict containing concept extraction metrics
        """
        metrics = {}
        
        # Extract concept texts for evaluation
        concept_texts = [concept.get("text", "") for concept in extracted_concepts]
        
        if reference_concepts:
            # Calculate precision, recall, F1
            precision, recall, f1 = self._calculate_concept_prf(concept_texts, reference_concepts)
            metrics["precision"] = precision
            metrics["recall"] = recall
            metrics["f1_score"] = f1
            
            # Calculate concept coverage
            coverage = self._calculate_concept_coverage(concept_texts, reference_concepts)
            metrics["concept_coverage"] = coverage
        
        # Quality indicators (reference-free)
        metrics.update(self._assess_concept_extraction_quality(extracted_concepts, original_text))
        
        return metrics
    
    def evaluate_summary_quality(self, generated_summary: str, 
                                reference_summary: str = None,
                                original_text: str = "") -> Dict[str, float]:
        """
        Evaluate summary generation quality.
        
        Args:
            generated_summary: Summary generated by the system
            reference_summary: Ground truth summary (if available)
            original_text: Original lecture text
            
        Returns:
            Dict containing summary quality metrics
        """
        metrics = {}
        
        if reference_summary:
            # ROUGE scores
            rouge_scores = self._calculate_rouge_scores(generated_summary, reference_summary)
            metrics.update(rouge_scores)
            
            # Semantic similarity
            semantic_sim = self._calculate_semantic_similarity(generated_summary, reference_summary)
            metrics["semantic_similarity"] = semantic_sim
        
        if original_text:
            # Compression ratio
            compression_ratio = len(generated_summary.split()) / len(original_text.split())
            metrics["compression_ratio"] = compression_ratio
            
            # Information retention
            info_retention = self._calculate_information_retention(generated_summary, original_text)
            metrics["information_retention"] = info_retention
        
        # Quality indicators (reference-free)
        metrics.update(self._assess_summary_quality(generated_summary))
        
        return metrics
    
    def evaluate_processing_performance(self, processing_times: Dict[str, float],
                                      system_metrics: Dict[str, Any]) -> Dict[str, float]:
        """
        Evaluate system performance metrics.
        
        Args:
            processing_times: Time taken for each processing stage
            system_metrics: Additional system performance data
            
        Returns:
            Dict containing performance metrics
        """
        metrics = {}
        
        # Latency metrics
        total_time = sum(processing_times.values())
        metrics["total_processing_time"] = total_time
        metrics["average_stage_time"] = total_time / len(processing_times) if processing_times else 0
        
        # Stage-specific performance
        for stage, duration in processing_times.items():
            metrics[f"{stage}_latency"] = duration
            
            # Check against targets
            target_key = f"target_{stage}_latency"
            if hasattr(config, target_key.upper()):
                target = getattr(config, target_key.upper())
                metrics[f"{stage}_meets_target"] = duration <= target
        
        # Overall latency target
        metrics["meets_latency_target"] = total_time <= config.TARGET_LATENCY
        
        # Throughput (if batch processing)
        if "batch_size" in system_metrics:
            batch_size = system_metrics["batch_size"]
            metrics["throughput"] = batch_size / total_time if total_time > 0 else 0
        
        return metrics
    
    def evaluate_overall_quality(self, workflow_result: Dict[str, Any]) -> Dict[str, float]:
        """
        Evaluate overall system quality across all dimensions.
        
        Args:
            workflow_result: Complete workflow result
            
        Returns:
            Dict containing overall quality metrics
        """
        overall_metrics = {}
        
        # Extract components
        transcription = workflow_result.get("transcription", "")
        concepts = workflow_result.get("concepts", [])
        summary = workflow_result.get("summary", "")
        processing_times = workflow_result.get("stage_durations", {})
        
        # Evaluate each component
        transcription_metrics = self.evaluate_transcription_accuracy(transcription)
        concept_metrics = self.evaluate_concept_extraction(concepts, original_text=transcription)
        summary_metrics = self.evaluate_summary_quality(summary, original_text=transcription)
        performance_metrics = self.evaluate_processing_performance(processing_times, workflow_result)
        
        # Aggregate scores
        overall_metrics["transcription_score"] = self._aggregate_component_score(transcription_metrics)
        overall_metrics["concept_extraction_score"] = self._aggregate_component_score(concept_metrics)
        overall_metrics["summary_quality_score"] = self._aggregate_component_score(summary_metrics)
        overall_metrics["performance_score"] = self._aggregate_component_score(performance_metrics)
        
        # Overall system score (weighted average)
        weights = {
            "transcription_score": 0.25,
            "concept_extraction_score": 0.30,
            "summary_quality_score": 0.30,
            "performance_score": 0.15
        }
        
        overall_score = sum(
            overall_metrics[metric] * weight 
            for metric, weight in weights.items()
        )
        overall_metrics["overall_system_score"] = overall_score
        
        # Quality thresholds check
        overall_metrics["meets_accuracy_target"] = overall_score >= config.TARGET_ACCURACY
        overall_metrics["meets_completeness_target"] = self._check_completeness(workflow_result)
        
        return overall_metrics
    
    def run_full_evaluation(self, test_cases: List[Dict] = None) -> Dict[str, Any]:
        """
        Run comprehensive evaluation across multiple test cases.
        
        Args:
            test_cases: List of test cases with inputs and expected outputs
            
        Returns:
            Dict containing full evaluation results
        """
        if test_cases is None:
            test_cases = self._create_default_test_cases()
        
        evaluation_results = {
            "timestamp": time.time(),
            "test_cases_count": len(test_cases),
            "individual_results": [],
            "aggregate_metrics": {},
            "performance_summary": {}
        }
        
        # Evaluate each test case
        for i, test_case in enumerate(test_cases):
            self.logger.info(f"Evaluating test case {i+1}/{len(test_cases)}")
            
            try:
                # Handle synthetic content differently
                if test_case.get("synthetic", False):
                    # For synthetic content, create a mock result
                    result = {
                        "transcription": test_case.get("content", ""),
                        "concepts": [{"concept": "test_concept", "importance": 0.8}],
                        "summary": f"Summary of {test_case.get('description', 'content')}",
                        "status": "success",
                        "processing_time": 1.0,
                        "confidence": 0.9
                    }
                    self.logger.info(f"Using synthetic content for test case {test_case['id']}")
                else:
                    # Run the system on actual audio input
                    from agents.orchestrator import LectureAgentOrchestrator
                    orchestrator = LectureAgentOrchestrator()
                    
                    result = orchestrator.process_lecture(test_case["input"])
                
                # Evaluate the result
                evaluation = self.evaluate_overall_quality(result)
                
                # Add test case metadata
                evaluation["test_case_id"] = test_case.get("id", i)
                evaluation["test_case_type"] = test_case.get("type", "unknown")
                
                evaluation_results["individual_results"].append(evaluation)
                
            except Exception as e:
                self.logger.error(f"Test case {i+1} failed: {e}")
                evaluation_results["individual_results"].append({
                    "test_case_id": test_case.get("id", i),
                    "error": str(e),
                    "failed": True
                })
        
        # Calculate aggregate metrics
        successful_results = [r for r in evaluation_results["individual_results"] if not r.get("failed", False)]
        
        if successful_results:
            evaluation_results["aggregate_metrics"] = self._calculate_aggregate_metrics(successful_results)
            evaluation_results["performance_summary"] = self._generate_performance_summary(successful_results)
        
        # Save evaluation results
        self._save_evaluation_results(evaluation_results)
        
        return evaluation_results
    
    def _calculate_wer(self, predicted: str, reference: str) -> float:
        """Calculate Word Error Rate."""
        pred_words = predicted.lower().split()
        ref_words = reference.lower().split()
        
        # Simple edit distance calculation
        d = np.zeros((len(ref_words) + 1, len(pred_words) + 1))
        
        for i in range(len(ref_words) + 1):
            d[i][0] = i
        for j in range(len(pred_words) + 1):
            d[0][j] = j
        
        for i in range(1, len(ref_words) + 1):
            for j in range(1, len(pred_words) + 1):
                if ref_words[i-1] == pred_words[j-1]:
                    d[i][j] = d[i-1][j-1]
                else:
                    d[i][j] = min(d[i-1][j], d[i][j-1], d[i-1][j-1]) + 1
        
        return d[len(ref_words)][len(pred_words)] / len(ref_words) if ref_words else 0
    
    def _calculate_semantic_similarity(self, text1: str, text2: str) -> float:
        """Calculate semantic similarity using sentence embeddings."""
        if not self.embedding_model or not text1.strip() or not text2.strip():
            return 0.0
        
        try:
            embeddings = self.embedding_model.encode([text1, text2])
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            return float(similarity)
        except Exception:
            return 0.0
    
    def _calculate_bleu_score(self, predicted: str, reference: str) -> float:
        """Calculate BLEU score for text similarity."""
        pred_tokens = predicted.lower().split()
        ref_tokens = reference.lower().split()
        
        if not pred_tokens or not ref_tokens:
            return 0.0
        
        # Simple 1-gram BLEU calculation
        pred_counts = defaultdict(int)
        ref_counts = defaultdict(int)
        
        for token in pred_tokens:
            pred_counts[token] += 1
        for token in ref_tokens:
            ref_counts[token] += 1
        
        overlap = 0
        for token, count in pred_counts.items():
            overlap += min(count, ref_counts[token])
        
        precision = overlap / len(pred_tokens) if pred_tokens else 0
        
        # Brevity penalty
        bp = min(1.0, len(pred_tokens) / len(ref_tokens)) if ref_tokens else 0
        
        return bp * precision
    
    def _assess_transcription_quality(self, text: str) -> Dict[str, float]:
        """Assess transcription quality without reference."""
        metrics = {}
        
        # Length indicators
        word_count = len(text.split())
        metrics["word_count"] = word_count
        metrics["appropriate_length"] = 1.0 if 100 <= word_count <= 5000 else 0.5
        
        # Completeness indicators
        sentence_count = len([s for s in text.split('.') if s.strip()])
        metrics["sentence_count"] = sentence_count
        metrics["avg_sentence_length"] = word_count / sentence_count if sentence_count > 0 else 0
        
        # Quality indicators
        metrics["contains_academic_terms"] = self._contains_academic_vocabulary(text)
        metrics["proper_punctuation"] = self._assess_punctuation_quality(text)
        
        return metrics
    
    def _calculate_concept_prf(self, predicted: List[str], reference: List[str]) -> Tuple[float, float, float]:
        """Calculate precision, recall, F1 for concept extraction."""
        if not predicted and not reference:
            return 1.0, 1.0, 1.0
        
        if not predicted:
            return 0.0, 0.0, 0.0
        
        if not reference:
            return 0.0, 1.0, 0.0
        
        # Normalize texts for comparison
        pred_normalized = {self._normalize_concept(c) for c in predicted}
        ref_normalized = {self._normalize_concept(c) for c in reference}
        
        # Calculate overlaps
        true_positives = len(pred_normalized.intersection(ref_normalized))
        precision = true_positives / len(pred_normalized)
        recall = true_positives / len(ref_normalized)
        
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        
        return precision, recall, f1
    
    def _calculate_concept_coverage(self, predicted: List[str], reference: List[str]) -> float:
        """Calculate what percentage of reference concepts are covered."""
        if not reference:
            return 1.0
        
        pred_normalized = {self._normalize_concept(c) for c in predicted}
        ref_normalized = {self._normalize_concept(c) for c in reference}
        
        covered = len(ref_normalized.intersection(pred_normalized))
        return covered / len(ref_normalized)
    
    def _assess_concept_extraction_quality(self, concepts: List[Dict], original_text: str) -> Dict[str, float]:
        """Assess concept extraction quality without reference."""
        metrics = {}
        
        if not concepts:
            return {"concept_count": 0, "extraction_quality": 0.0}
        
        # Count and diversity metrics
        metrics["concept_count"] = len(concepts)
        metrics["appropriate_concept_count"] = 1.0 if 3 <= len(concepts) <= 20 else 0.5
        
        # Quality distribution
        importance_scores = [c.get("importance", 0.5) for c in concepts]
        metrics["avg_importance"] = np.mean(importance_scores)
        metrics["importance_variance"] = np.var(importance_scores)
        
        # Type diversity
        concept_types = [c.get("type", "unknown") for c in concepts]
        unique_types = len(set(concept_types))
        metrics["type_diversity"] = unique_types / len(concepts)
        
        # Content quality
        concept_texts = [c.get("text", "") for c in concepts]
        metrics["avg_concept_length"] = np.mean([len(text.split()) for text in concept_texts])
        
        return metrics
    
    def _calculate_rouge_scores(self, predicted: str, reference: str) -> Dict[str, float]:
        """Calculate ROUGE scores for summary evaluation."""
        scores = {}
        
        # ROUGE-1 (unigram overlap)
        pred_unigrams = set(predicted.lower().split())
        ref_unigrams = set(reference.lower().split())
        
        if ref_unigrams:
            rouge_1_recall = len(pred_unigrams.intersection(ref_unigrams)) / len(ref_unigrams)
            rouge_1_precision = len(pred_unigrams.intersection(ref_unigrams)) / len(pred_unigrams) if pred_unigrams else 0
            rouge_1_f1 = 2 * rouge_1_precision * rouge_1_recall / (rouge_1_precision + rouge_1_recall) if (rouge_1_precision + rouge_1_recall) > 0 else 0
            
            scores["rouge_1_recall"] = rouge_1_recall
            scores["rouge_1_precision"] = rouge_1_precision
            scores["rouge_1_f1"] = rouge_1_f1
        
        return scores
    
    def _calculate_information_retention(self, summary: str, original: str) -> float:
        """Calculate how much information from original is retained in summary."""
        if not self.embedding_model:
            return 0.5  # Default score
        
        try:
            # Split into chunks for comparison
            original_chunks = self._split_into_chunks(original, chunk_size=200)
            summary_embedding = self.embedding_model.encode([summary])
            
            # Find best matching chunks
            similarities = []
            for chunk in original_chunks:
                chunk_embedding = self.embedding_model.encode([chunk])
                sim = cosine_similarity(summary_embedding, chunk_embedding)[0][0]
                similarities.append(sim)
            
            # Return average of top similarities
            top_similarities = sorted(similarities, reverse=True)[:3]
            return np.mean(top_similarities) if top_similarities else 0.0
            
        except Exception:
            return 0.5
    
    def _assess_summary_quality(self, summary: str) -> Dict[str, float]:
        """Assess summary quality without reference."""
        metrics = {}
        
        # Length and structure
        word_count = len(summary.split())
        sentence_count = len([s for s in summary.split('.') if s.strip()])
        
        metrics["word_count"] = word_count
        metrics["sentence_count"] = sentence_count
        metrics["appropriate_length"] = 1.0 if 100 <= word_count <= 1000 else 0.5
        
        # Structure quality
        metrics["has_headers"] = 1.0 if any(line.startswith('#') for line in summary.split('\n')) else 0.0
        metrics["has_bullet_points"] = 1.0 if '-' in summary or '*' in summary else 0.0
        metrics["has_formatting"] = 1.0 if '**' in summary or '__' in summary else 0.0
        
        # Content quality
        metrics["coherence"] = self._assess_coherence(summary)
        metrics["informativeness"] = self._assess_informativeness(summary)
        
        return metrics
    
    def _aggregate_component_score(self, component_metrics: Dict[str, float]) -> float:
        """Aggregate individual metrics into a component score."""
        if not component_metrics:
            return 0.0
        
        # Filter numeric metrics only
        numeric_metrics = {k: v for k, v in component_metrics.items() 
                          if isinstance(v, (int, float)) and 0 <= v <= 1}
        
        if not numeric_metrics:
            return 0.5  # Default score
        
        return np.mean(list(numeric_metrics.values()))
    
    def _check_completeness(self, workflow_result: Dict[str, Any]) -> bool:
        """Check if the workflow result meets completeness requirements."""
        required_components = ["transcription", "concepts", "summary"]
        
        for component in required_components:
            if not workflow_result.get(component):
                return False
        
        # Check minimum thresholds
        concept_count = len(workflow_result.get("concepts", []))
        summary_length = len(workflow_result.get("summary", "").split())
        
        return concept_count >= 3 and summary_length >= 100
    
    def _create_default_test_cases(self) -> List[Dict]:
        """Create default test cases for evaluation."""
        # Check for existing audio files in the data/audio directory
        audio_dir = "data/audio"
        test_cases = []
        
        # Look for any existing audio files
        if os.path.exists(audio_dir):
            audio_files = [f for f in os.listdir(audio_dir) if f.endswith(('.mp3', '.wav', '.m4a', '.flac'))]
            
            for i, audio_file in enumerate(audio_files[:2]):  # Use up to 2 files for testing
                test_cases.append({
                    "id": f"test_{i+1:03d}",
                    "type": "mixed_content",
                    "input": os.path.join(audio_dir, audio_file),
                    "description": f"Audio lecture from {audio_file}"
                })
        
        # If no audio files found, create synthetic test cases
        if not test_cases:
            self.logger.warning("No audio files found, creating synthetic test cases")
            test_cases = [
                {
                    "id": "test_001",
                    "type": "mathematics",
                    "input": "synthetic_math_content",  # Will be handled as text
                    "description": "Synthetic mathematics lecture content",
                    "synthetic": True,
                    "content": "Today we will discuss calculus fundamentals including derivatives and integrals. The derivative of x squared is 2x."
                },
                {
                    "id": "test_002", 
                    "type": "computer_science",
                    "input": "synthetic_cs_content",
                    "description": "Synthetic computer science lecture content",
                    "synthetic": True,
                    "content": "We will explore sorting algorithms including quicksort and mergesort. Big O notation is crucial for algorithm analysis."
                }
            ]
        
        return test_cases
    
    def _calculate_aggregate_metrics(self, results: List[Dict]) -> Dict[str, float]:
        """Calculate aggregate metrics across test results."""
        if not results:
            return {}
        
        aggregate = {}
        
        # Collect all numeric metrics
        all_metrics = defaultdict(list)
        for result in results:
            for key, value in result.items():
                if isinstance(value, (int, float)) and not key.endswith("_id"):
                    all_metrics[key].append(value)
        
        # Calculate statistics
        for metric, values in all_metrics.items():
            aggregate[f"avg_{metric}"] = np.mean(values)
            aggregate[f"std_{metric}"] = np.std(values)
            aggregate[f"min_{metric}"] = np.min(values)
            aggregate[f"max_{metric}"] = np.max(values)
        
        return aggregate
    
    def _generate_performance_summary(self, results: List[Dict]) -> Dict[str, Any]:
        """Generate performance summary report."""
        summary = {
            "total_tests": len(results),
            "successful_tests": len([r for r in results if not r.get("failed", False)]),
            "average_overall_score": np.mean([r.get("overall_system_score", 0) for r in results]),
            "tests_meeting_accuracy_target": len([r for r in results if r.get("meets_accuracy_target", False)]),
            "tests_meeting_completeness_target": len([r for r in results if r.get("meets_completeness_target", False)])
        }
        
        summary["success_rate"] = summary["successful_tests"] / summary["total_tests"]
        summary["accuracy_target_rate"] = summary["tests_meeting_accuracy_target"] / summary["total_tests"]
        summary["completeness_target_rate"] = summary["tests_meeting_completeness_target"] / summary["total_tests"]
        
        return summary
    
    def _save_evaluation_results(self, results: Dict[str, Any]):
        """Save evaluation results to file."""
        try:
            os.makedirs("evaluation/results", exist_ok=True)
            
            timestamp = int(time.time())
            filename = f"evaluation_results_{timestamp}.json"
            filepath = os.path.join("evaluation/results", filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
            
            self.logger.info(f"Evaluation results saved to: {filepath}")
            
        except Exception as e:
            self.logger.error(f"Failed to save evaluation results: {e}")
    
    # Helper methods
    def _normalize_concept(self, concept: str) -> str:
        """Normalize concept text for comparison."""
        return re.sub(r'[^\w\s]', '', concept.lower().strip())
    
    def _contains_academic_vocabulary(self, text: str) -> float:
        """Check if text contains academic vocabulary."""
        academic_terms = [
            "theorem", "hypothesis", "analysis", "synthesis", "methodology",
            "paradigm", "concept", "principle", "formula", "equation"
        ]
        
        text_lower = text.lower()
        found_terms = sum(1 for term in academic_terms if term in text_lower)
        return min(found_terms / 5.0, 1.0)  # Normalize to 0-1
    
    def _assess_punctuation_quality(self, text: str) -> float:
        """Assess punctuation quality in text."""
        sentences = text.split('.')
        properly_punctuated = sum(1 for s in sentences if s.strip() and s.strip()[-1] in '.!?')
        return properly_punctuated / len(sentences) if sentences else 0.0
    
    def _split_into_chunks(self, text: str, chunk_size: int = 200) -> List[str]:
        """Split text into chunks of specified word size."""
        words = text.split()
        chunks = []
        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
        return chunks
    
    def _assess_coherence(self, text: str) -> float:
        """Assess coherence of the text."""
        # Simple coherence metrics
        sentences = [s.strip() for s in text.split('.') if s.strip()]
        
        if len(sentences) < 2:
            return 0.5
        
        # Check for transition words
        transition_words = ['however', 'therefore', 'furthermore', 'moreover', 'additionally']
        has_transitions = any(word in text.lower() for word in transition_words)
        
        # Check for consistent structure
        consistent_structure = text.count('#') > 0 or text.count('-') > 2
        
        return (0.5 + (0.25 if has_transitions else 0) + (0.25 if consistent_structure else 0))
    
    def _assess_informativeness(self, text: str) -> float:
        """Assess informativeness of the text."""
        # Check for key information indicators
        info_indicators = [
            'definition', 'formula', 'example', 'principle', 'law',
            'theorem', 'concept', 'method', 'process', 'technique'
        ]
        
        text_lower = text.lower()
        found_indicators = sum(1 for indicator in info_indicators if indicator in text_lower)
        
        return min(found_indicators / 5.0, 1.0)